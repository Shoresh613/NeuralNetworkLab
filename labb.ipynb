{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FrameStack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this to have in memory, as importing FrameStack didn't work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Wrapper that stacks frames.\"\"\"\n",
    "from collections import deque\n",
    "from typing import Union\n",
    "\n",
    "import numpy as np\n",
    "# !pip install gymnasium[accept-rom-license]\n",
    "# !pip install gymnasium[atari]==0.28.1 ale-py==0.8.1\n",
    "# !pip install gymnasium[other]==0.28.1\n",
    "import gymnasium as gym\n",
    "from gymnasium.error import DependencyNotInstalled\n",
    "from gymnasium.spaces import Box\n",
    "\n",
    "\n",
    "class LazyFrames:\n",
    "    \"\"\"Ensures common frames are only stored once to optimize memory use.\n",
    "\n",
    "    To further reduce the memory use, it is optionally to turn on lz4 to compress the observations.\n",
    "\n",
    "    Note:\n",
    "        This object should only be converted to numpy array just before forward pass.\n",
    "    \"\"\"\n",
    "\n",
    "    __slots__ = (\"frame_shape\", \"dtype\", \"shape\", \"lz4_compress\", \"_frames\")\n",
    "\n",
    "    def __init__(self, frames: list, lz4_compress: bool = False):\n",
    "        \"\"\"Lazyframe for a set of frames and if to apply lz4.\n",
    "\n",
    "        Args:\n",
    "            frames (list): The frames to convert to lazy frames\n",
    "            lz4_compress (bool): Use lz4 to compress the frames internally\n",
    "\n",
    "        Raises:\n",
    "            DependencyNotInstalled: lz4 is not installed\n",
    "        \"\"\"\n",
    "        self.frame_shape = tuple(frames[0].shape)\n",
    "        self.shape = (len(frames),) + self.frame_shape\n",
    "        self.dtype = frames[0].dtype\n",
    "        if lz4_compress:\n",
    "            try:\n",
    "                from lz4.block import compress\n",
    "            except ImportError as e:\n",
    "                raise DependencyNotInstalled(\n",
    "                    \"lz4 is not installed, run `pip install gymnasium[other]`\"\n",
    "                ) from e\n",
    "\n",
    "            frames = [compress(frame) for frame in frames]\n",
    "        self._frames = frames\n",
    "        self.lz4_compress = lz4_compress\n",
    "\n",
    "    def __array__(self, dtype=None):\n",
    "        \"\"\"Gets a numpy array of stacked frames with specific dtype.\n",
    "\n",
    "        Args:\n",
    "            dtype: The dtype of the stacked frames\n",
    "\n",
    "        Returns:\n",
    "            The array of stacked frames with dtype\n",
    "        \"\"\"\n",
    "        arr = self[:]\n",
    "        if dtype is not None:\n",
    "            return arr.astype(dtype)\n",
    "        return arr\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the number of frame stacks.\n",
    "\n",
    "        Returns:\n",
    "            The number of frame stacks\n",
    "        \"\"\"\n",
    "        return self.shape[0]\n",
    "\n",
    "    def __getitem__(self, int_or_slice: Union[int, slice]):\n",
    "        \"\"\"Gets the stacked frames for a particular index or slice.\n",
    "\n",
    "        Args:\n",
    "            int_or_slice: Index or slice to get items for\n",
    "\n",
    "        Returns:\n",
    "            np.stacked frames for the int or slice\n",
    "\n",
    "        \"\"\"\n",
    "        if isinstance(int_or_slice, int):\n",
    "            return self._check_decompress(self._frames[int_or_slice])  # single frame\n",
    "        return np.stack(\n",
    "            [self._check_decompress(f) for f in self._frames[int_or_slice]], axis=0\n",
    "        )\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        \"\"\"Checks that the current frames are equal to the other object.\"\"\"\n",
    "        return self.__array__() == other\n",
    "\n",
    "    def _check_decompress(self, frame):\n",
    "        if self.lz4_compress:\n",
    "            from lz4.block import decompress\n",
    "\n",
    "            return np.frombuffer(decompress(frame), dtype=self.dtype).reshape(\n",
    "                self.frame_shape\n",
    "            )\n",
    "        return frame\n",
    "\n",
    "\n",
    "class FrameStack(gym.ObservationWrapper, gym.utils.RecordConstructorArgs):\n",
    "    \"\"\"Observation wrapper that stacks the observations in a rolling manner.\n",
    "\n",
    "    For example, if the number of stacks is 4, then the returned observation contains\n",
    "    the most recent 4 observations. For environment 'Pendulum-v1', the original observation\n",
    "    is an array with shape [3], so if we stack 4 observations, the processed observation\n",
    "    has shape [4, 3].\n",
    "\n",
    "    Note:\n",
    "        - To be memory efficient, the stacked observations are wrapped by :class:`LazyFrame`.\n",
    "        - The observation space must be :class:`Box` type. If one uses :class:`Dict`\n",
    "          as observation space, it should apply :class:`FlattenObservation` wrapper first.\n",
    "        - After :meth:`reset` is called, the frame buffer will be filled with the initial observation.\n",
    "          I.e. the observation returned by :meth:`reset` will consist of `num_stack` many identical frames.\n",
    "\n",
    "    Example:\n",
    "        >>> import gymnasium as gym\n",
    "        >>> from gymnasium.wrappers import FrameStack\n",
    "        >>> env = gym.make(\"CarRacing-v2\")\n",
    "        >>> env = FrameStack(env, 4)\n",
    "        >>> env.observation_space\n",
    "        Box(0, 255, (4, 96, 96, 3), uint8)\n",
    "        >>> obs, _ = env.reset()\n",
    "        >>> obs.shape\n",
    "        (4, 96, 96, 3)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        env: gym.Env,\n",
    "        num_stack: int,\n",
    "        lz4_compress: bool = False,\n",
    "    ):\n",
    "        \"\"\"Observation wrapper that stacks the observations in a rolling manner.\n",
    "\n",
    "        Args:\n",
    "            env (Env): The environment to apply the wrapper\n",
    "            num_stack (int): The number of frames to stack\n",
    "            lz4_compress (bool): Use lz4 to compress the frames internally\n",
    "        \"\"\"\n",
    "        gym.utils.RecordConstructorArgs.__init__(\n",
    "            self, num_stack=num_stack, lz4_compress=lz4_compress\n",
    "        )\n",
    "        gym.ObservationWrapper.__init__(self, env)\n",
    "\n",
    "        self.num_stack = num_stack\n",
    "        self.lz4_compress = lz4_compress\n",
    "\n",
    "        self.frames = deque(maxlen=num_stack)\n",
    "\n",
    "        low = np.repeat(self.observation_space.low[np.newaxis, ...], num_stack, axis=0)\n",
    "        high = np.repeat(\n",
    "            self.observation_space.high[np.newaxis, ...], num_stack, axis=0\n",
    "        )\n",
    "        self.observation_space = Box(\n",
    "            low=low, high=high, dtype=self.observation_space.dtype\n",
    "        )\n",
    "\n",
    "    def observation(self, observation):\n",
    "        \"\"\"Converts the wrappers current frames to lazy frames.\n",
    "\n",
    "        Args:\n",
    "            observation: Ignored\n",
    "\n",
    "        Returns:\n",
    "            :class:`LazyFrames` object for the wrapper's frame buffer,  :attr:`self.frames`\n",
    "        \"\"\"\n",
    "        assert len(self.frames) == self.num_stack, (len(self.frames), self.num_stack)\n",
    "        return LazyFrames(list(self.frames), self.lz4_compress)\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Steps through the environment, appending the observation to the frame buffer.\n",
    "\n",
    "        Args:\n",
    "            action: The action to step through the environment with\n",
    "\n",
    "        Returns:\n",
    "            Stacked observations, reward, terminated, truncated, and information from the environment\n",
    "        \"\"\"\n",
    "        observation, reward, terminated, truncated, info = self.env.step(action)\n",
    "        self.frames.append(observation)\n",
    "        return self.observation(None), reward, terminated, truncated, info\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        \"\"\"Reset the environment with kwargs.\n",
    "\n",
    "        Args:\n",
    "            **kwargs: The kwargs for the environment reset\n",
    "\n",
    "        Returns:\n",
    "            The stacked observations\n",
    "        \"\"\"\n",
    "        obs, info = self.env.reset(**kwargs)\n",
    "\n",
    "        [self.frames.append(obs) for _ in range(self.num_stack)]\n",
    "\n",
    "        return self.observation(None), info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code didn't learn well, the score didn't improve during the 6000 episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.comNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: gymnasium[accept-rom-license] in c:\\users\\mikae\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.0.0)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\mikae\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gymnasium[accept-rom-license]) (1.24.3)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\mikae\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gymnasium[accept-rom-license]) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\mikae\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gymnasium[accept-rom-license]) (4.12.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\mikae\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gymnasium[accept-rom-license]) (0.0.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: gymnasium 1.0.0 does not provide the extra 'accept-rom-license'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prefilling replay buffer with 16000 transitions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filling Replay Buffer: 100%|██████████| 16000/16000 [00:32<00:00, 486.53it/s]\n",
      "C:\\Users\\Mikae\\AppData\\Local\\Temp\\ipykernel_9520\\1019031436.py:176: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replay buffer prefilled.\n",
      "No checkpoint found, starting from scratch.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mikae\\AppData\\Local\\Temp\\ipykernel_9520\\1019031436.py:226: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/10000, Total Reward: 80.0, Epsilon: 0.24, Time: 22.84s\n",
      "Episode 2/10000, Total Reward: 155.0, Epsilon: 0.10, Time: 21.07s\n",
      "Episode 3/10000, Total Reward: 110.0, Epsilon: 0.10, Time: 25.01s\n",
      "Episode 4/10000, Total Reward: 210.0, Epsilon: 0.10, Time: 20.75s\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "%pip install gymnasium[accept-rom-license]\n",
    "import gymnasium as gym\n",
    "import ale_py\n",
    "import numpy as np\n",
    "import random\n",
    "import threading\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "from collections import deque\n",
    "from queue import Queue\n",
    "from gymnasium.wrappers.atari_preprocessing import AtariPreprocessing\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Benchmark mode in CuDNN for potentially faster convolutions\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, num_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(4, 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, num_actions)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "        self.lock = threading.Lock()\n",
    "            \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        with self.lock:\n",
    "            self.buffer.append((state, action, reward, next_state, done))\n",
    "                \n",
    "    def sample(self, batch_size):\n",
    "        with self.lock:\n",
    "            return random.sample(self.buffer, batch_size)\n",
    "                            \n",
    "    def __len__(self):\n",
    "        with self.lock:\n",
    "            return len(self.buffer)\n",
    "\n",
    "\n",
    "def prefetch_batches(memory, batch_size, prefetch_queue, stop_event):\n",
    "    while not stop_event.is_set():\n",
    "        if len(memory) >= batch_size:\n",
    "            batch = memory.sample(batch_size)\n",
    "            prefetch_queue.put(batch)\n",
    "        else:\n",
    "            stop_event.wait(0.1)\n",
    "\n",
    "\n",
    "def save_checkpoint(model, optimizer, episode, epsilon, total_steps, checkpoint_path=\"checkpoint.pth\"):\n",
    "    print(\"Starting torch.save for checkpoint (no replay buffer)...\")\n",
    "    checkpoint = {\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"episode\": episode,\n",
    "        \"epsilon\": epsilon,\n",
    "        \"total_steps\": total_steps\n",
    "    }\n",
    "    torch.save(checkpoint, checkpoint_path)\n",
    "    print(f\"Checkpoint saved at {checkpoint_path}\")\n",
    "\n",
    "def load_checkpoint(model, optimizer, memory, checkpoint_path=\"checkpoint.pth\"):\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "        episode = checkpoint[\"episode\"]\n",
    "        epsilon = checkpoint[\"epsilon\"]\n",
    "        total_steps = checkpoint[\"total_steps\"]\n",
    "        print(f\"Checkpoint loaded from {checkpoint_path}\")\n",
    "        return episode, epsilon, total_steps\n",
    "    else:\n",
    "        print(\"No checkpoint found, starting from scratch.\")\n",
    "        return 0, 1.0, 0\n",
    "\n",
    "\n",
    "def prefill_replay_buffer(env, memory, prefill_size):\n",
    "    print(f\"Prefilling replay buffer with {prefill_size} transitions...\")\n",
    "    state, _ = env.reset()\n",
    "    for _ in tqdm(range(prefill_size), desc=\"Filling Replay Buffer\"):\n",
    "        action = env.action_space.sample()\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        state_array = np.array(state)\n",
    "        next_state_array = np.array(next_state)\n",
    "\n",
    "        # Kontrollera format: Om (H, W, C), transponera till (C, H, W)\n",
    "        if state_array.shape == (84, 84, 4):\n",
    "            state_array = state_array.transpose(2, 0, 1)\n",
    "        if next_state_array.shape == (84, 84, 4):\n",
    "            next_state_array = next_state_array.transpose(2, 0, 1)\n",
    "\n",
    "        memory.push(state_array, action, reward, next_state_array, done)\n",
    "\n",
    "        if done:\n",
    "            state, _ = env.reset()\n",
    "        else:\n",
    "            state = next_state\n",
    "    print(\"Replay buffer prefilled.\")\n",
    "\n",
    "num_actions = 6\n",
    "batch_size = 16   \n",
    "gamma = 0.99\n",
    "epsilon_start = 1.0\n",
    "epsilon_end = 0.1\n",
    "epsilon_decay = 0.995\n",
    "learning_rate = 1e-4\n",
    "target_update_freq = 1000\n",
    "memory_size = 100000\n",
    "num_episodes = 10000\n",
    "prefetch_queue_size = 20\n",
    "save_frequency = 50\n",
    "updates_per_step = 2  \n",
    "\n",
    "env = gym.make(\"SpaceInvadersNoFrameskip-v4\")\n",
    "env = AtariPreprocessing(env)\n",
    "env = FrameStack(env, 4)\n",
    "\n",
    "policy_net = DQN(num_actions).to(device)\n",
    "target_net = DQN(num_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate)\n",
    "\n",
    "memory = ReplayBuffer(memory_size)\n",
    "\n",
    "prefill_size = max(batch_size * 1000, batch_size)\n",
    "prefill_replay_buffer(env, memory, prefill_size)\n",
    "\n",
    "prefetch_queue = Queue(maxsize=prefetch_queue_size)\n",
    "stop_event = threading.Event()\n",
    "prefetch_thread = threading.Thread(target=prefetch_batches, args=(memory, batch_size, prefetch_queue, stop_event))\n",
    "prefetch_thread.start()\n",
    "\n",
    "\n",
    "start_episode, epsilon, total_steps = load_checkpoint(policy_net, optimizer, memory, \"checkpoint.pth\")\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "try:\n",
    "    for episode in range(start_episode, num_episodes):\n",
    "        start_time = time.time()\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            state_array = np.array(state)\n",
    "            if state_array.shape == (84, 84, 4):\n",
    "                state_array = state_array.transpose(2, 0, 1)\n",
    "\n",
    "            if random.random() < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                state_tensor = torch.tensor(state_array, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "                with torch.no_grad():\n",
    "                    action_values = policy_net(state_tensor)\n",
    "                action = torch.argmax(action_values, dim=1).item()\n",
    "\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            total_reward += reward\n",
    "\n",
    "            next_state_array = np.array(next_state)\n",
    "            if next_state_array.shape == (84, 84, 4):\n",
    "                next_state_array = next_state_array.transpose(2, 0, 1)\n",
    "\n",
    "            memory.push(state_array, action, reward, next_state_array, done)\n",
    "\n",
    "            for _ in range(updates_per_step):\n",
    "                if not prefetch_queue.empty():\n",
    "                    batch = prefetch_queue.get()\n",
    "                    prefetch_queue.task_done()\n",
    "\n",
    "                    states, actions, rewards, next_states, dones = zip(*batch)\n",
    "                    states = torch.tensor(np.array(states), dtype=torch.float32).to(device)\n",
    "                    actions = torch.tensor(np.array(actions), dtype=torch.int64).unsqueeze(1).to(device)\n",
    "                    rewards = torch.tensor(np.array(rewards), dtype=torch.float32).unsqueeze(1).to(device)\n",
    "                    next_states = torch.tensor(np.array(next_states), dtype=torch.float32).to(device)\n",
    "                    dones = torch.tensor(np.array(dones), dtype=torch.float32).unsqueeze(1).to(device)\n",
    "\n",
    "                    # Double DQN logic\n",
    "                    with torch.no_grad():\n",
    "                        next_actions = policy_net(next_states).argmax(dim=1, keepdim=True)\n",
    "                        next_q_values = target_net(next_states).gather(1, next_actions)\n",
    "                    target_q_values = rewards + gamma * next_q_values * (1 - dones)\n",
    "\n",
    "                    with torch.cuda.amp.autocast():\n",
    "                        q_values = policy_net(states).gather(1, actions)\n",
    "                        loss = F.smooth_l1_loss(q_values, target_q_values)\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    scaler.scale(loss).backward()\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "\n",
    "            epsilon = max(epsilon_end, epsilon_decay * epsilon)\n",
    "            if total_steps % target_update_freq == 0:\n",
    "                target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "            state = next_state\n",
    "            total_steps += 1\n",
    "\n",
    "        if episode % 10 == 0:\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        if (episode + 1) % save_frequency == 0:\n",
    "            save_checkpoint(policy_net, optimizer, episode, epsilon, total_steps)\n",
    "\n",
    "        episode_time = time.time() - start_time\n",
    "        print(f\"Episode {episode+1}/{num_episodes}, Total Reward: {total_reward}, Epsilon: {epsilon:.2f}, Time: {episode_time:.2f}s\")\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Training interrupted by user.\")\n",
    "finally:\n",
    "    stop_event.set()\n",
    "    prefetch_thread.join()\n",
    "    # Save final checkpoint without replay buffer\n",
    "    save_checkpoint(policy_net, optimizer, num_episodes, epsilon, total_steps)\n",
    "    torch.save(policy_net.state_dict(), 'dqn_space_invaders_final.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mikae\\AppData\\Local\\Temp\\ipykernel_8140\\826240131.py:30: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  agent.load_state_dict(torch.load('dqn_space_invaders.pth', map_location=device))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DQN(\n",
       "  (conv1): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
       "  (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (fc1): Linear(in_features=3136, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Number of actions in Space Invaders\n",
    "num_actions = 6\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, num_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(4, 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 512)\n",
    "        self.fc2 = nn.Linear(512, num_actions)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))   # (batch_size, 32, 20, 20)\n",
    "        x = F.relu(self.conv2(x))   # (batch_size, 64, 9, 9)\n",
    "        x = F.relu(self.conv3(x))   # (batch_size, 64, 7, 7)\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model and load the trained weights\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "agent = DQN(num_actions).to(device)\n",
    "agent.load_state_dict(torch.load('dqn_space_invaders.pth', map_location=device))\n",
    "agent.eval()  # Set the model to evaluation mode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up env with rendering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import ale_py\n",
    "from gymnasium.wrappers.atari_preprocessing import AtariPreprocessing\n",
    "\n",
    "# Create the environment with rendering enabled\n",
    "env = gym.make(\"SpaceInvadersNoFrameskip-v4\", render_mode=\"human\")\n",
    "env = AtariPreprocessing(env)\n",
    "env = FrameStack(env, num_stack=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the agent in the env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "state, _ = env.reset()\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        action_values = agent(state_tensor)\n",
    "        action = torch.argmax(action_values, dim=1).item()\n",
    "    \n",
    "    next_state, reward, done, _, _ = env.step(action)\n",
    "    \n",
    "    state = next_state\n",
    "    \n",
    "    env.render()\n",
    "    \n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with recording\n",
    "\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "\n",
    "env = gym.make(\"SpaceInvadersNoFrameskip-v4\")\n",
    "env = AtariPreprocessing(env)\n",
    "env = FrameStack(env, 4)\n",
    "\n",
    "# Wrap the environment to record videos\n",
    "env = RecordVideo(env, video_folder='videos', episode_trigger=lambda x: True)\n",
    "\n",
    "state, _ = env.reset()\n",
    "done = False\n",
    "total_reward = 0\n",
    "\n",
    "while not done:\n",
    "    state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Get action from the trained model\n",
    "        action_values = agent(state_tensor)\n",
    "        action = torch.argmax(action_values, dim=1).item()\n",
    "    \n",
    "    next_state, reward, done, _, _ = env.step(action)\n",
    "    total_reward += reward\n",
    "    state = next_state\n",
    "\n",
    "env.close()\n",
    "print(f\"Total Reward: {total_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "# Function to display the video\n",
    "def show_video():\n",
    "    video_files = [f for f in os.listdir('./videos') if f.endswith('.mp4')]\n",
    "    if len(video_files) > 0:\n",
    "        video_path = os.path.join('./videos', video_files[-1])\n",
    "        video = io.open(video_path, 'r+b').read()\n",
    "        encoded = base64.b64encode(video)\n",
    "        display(HTML(data='''\n",
    "            <video width=\"480\" height=\"320\" controls>\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "            </video>\n",
    "            '''.format(encoded.decode('ascii'))))\n",
    "    else:\n",
    "        print(\"No video found.\")\n",
    "\n",
    "# Display the recorded video\n",
    "show_video()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
