{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networking Lab report\n",
    "This was a course in TensorFlow, a library that unfortunately has been utterly broken, as evidenced during the code alongs of the course. This in conjunction with it not supporting GPU on Windows made me want to try doing the lab in PyTorch instead.\n",
    "\n",
    "It has been an interesting journey, as the template that we got for developing the model was written for TensorFlow, so it took some trial and error to get it to train properly on my GPU. This report will detail the different approaches and considerations made to try to get the model to improve in chronological order.\n",
    "\n",
    "## Local training\n",
    "\n",
    "Initially I trained the model for 6.000 episodes of gameplay, but it didn't learn anything. In disappointment I threw away the results and the model, which in hindsight might have been a bit rash. A nifty diagram would have been nice to display, but, you live and you learn.\n",
    "\n",
    "After feedback from the teacher, I adjusted the epsilon decay to reach its minimum after 1.000.000 frames (instead of basically during prefilling of the buffer), and performing training of the policy network on the whole replay buffer during the prefilling of the replay buffer, and increasing the batch size to 32 from 16. \n",
    "\n",
    "Unfortunately, some change was introduced that made the code freeze for some reason or another on my local computer, which I didn't quite manage to identify, which made me give up the idea of running the training locally. I bought 100 hours worth of compute at Google Colab, thereby increasing the GPU memory from 2GB to 26GB, RAM from 16GB to 64GB, and getting access to a more modern architecture, supporting for instance 16 bit fp operations (an L4 GPU). \n",
    "\n",
    "## Google Colab\n",
    "\n",
    "After having decided to move the updated training onto Google Colab, below are the initial results after 2000 episodes. The code, which ran w/o any problems in the cloud as opposed to locally, was adjusted to make use of the additional resources in terms of RAM and GPU memory, such as moving the replay buffer to the GPU while increasing its size, calculating additional gradients per step, and a larger batch size. \n",
    "\n",
    "![After 2000 episodes of training we notice learning happening in the beginning, but then a plateu ](imgs/2000episodesColab.png)\n",
    "\n",
    "As can be seen, there was some learning going on the first 100 episodes or so, when a dominant strategy seems to have been found, indicated by what is almost visible as a yellow line, that constantly led to 200 points. \n",
    "\n",
    "- Slope: 0.0489\n",
    "- R²: 0.0477 (low, suggesting the trend explains less than 5% of the variability)\n",
    "\n",
    "Not very impressive results, so the training was terminated again, in hope of finding a cure. (In retrospect, this turned out to be the best version of the model after all).\n",
    "\n",
    "#### Adjusting hyperparameters\n",
    "Next try, after these changes:\n",
    "\n",
    "- Adjust rewards dynamically to emphasize achieving higher scores.\n",
    "- Implement prioritized experience replay to focus on transitions with higher learning potential - Temporal Difference (TD) error.\n",
    "- Tried optimizer AdamW for better optimization.\n",
    "- Lowered the epsilon decay to reach its minimum after 2 million frames to keep exploring more.\n",
    "- Increased batch size to 64\n",
    "\n",
    "![After 1500 episodes of training we notice this didn't work as it's even worse than before, no learning at all basically ](imgs/1500episodes.png)\n",
    "\n",
    "After 1500 episodes of training we notice this didn't work as it's even worse than before, no learning at all basically, with both a slope and an R² value close to 0. Also, the average score is significantly worse than before, around half.\n",
    "\n",
    "#### Adding more layers\n",
    "Suspecting there's an underfit, so adding more layers to capture more complexity:\n",
    "- Added a fourth convolutional layer with 128 filters.\n",
    "- Increased the size of the dense layers for higher capacity.\n",
    "- Added an extra dense layer for better feature extraction.\n",
    "- Increased to 8 updates per step to see if that is more efficient.\n",
    "\n",
    "As there are 4 times the number of gradient updates per timestep the number of steps and episodes needed is presumed to be a lot less, so evaluation happens earlier this time, episode-wise.\n",
    "\n",
    "![After 150 episodes of training we notice this didn't work as it's yet again even worse than before, no learning at all basically and very low scores](imgs/150episodes8stepsMoreLayers.png)\n",
    "\n",
    "Apalling results, what can I say.\n",
    "\n",
    "#### Revert to original layers, hyperparameters altered\n",
    "\n",
    "As the results are apalling, reverting to the original layers, and hope the adjusted learning rate will do something to improve the learning. The learning rate has been reduced 10x to allow finer adjustments, and regularization has been introduced using weight_decay. Gradient clipping has also been added to prevent overshooting during training.\n",
    "Also updating target network more frequently to see if that helps (every 500 frames instead of every 1000).\n",
    "\n",
    "Started inspecting the logs regarding how the loss and the score develops over time, and use that as basis for adjustment of hyperparameters etc.\n",
    "\n",
    "![Reverted layers and decreased learning rate](imgs/210episodesRevertedLayers.png)\n",
    "\n",
    "Still it plateus around the score 150, definitely not satisfactory. Also the loss got out of hand increasing to over 100 at the later stages. Also, the score fluctuates. Combined this suggests the learning rate might be too high, and a need for adding regularization and further decreasing the learning rate. \n",
    "\n",
    "#### Adding Dropout and further decreasing learning rate \n",
    "\n",
    "- Try to implement a smoothed loss metric (moving average) to track training progress more effectively.\n",
    "- Adding two dropout layers with 20% dropout each to the model to mitigate suspected overfitting.\n",
    "- The learning rate has been reduced further\n",
    "- Gradient clipping has been adjusted to a stricter cap\n",
    "- The target network update frequency has been increased further to try to improve stability.\n",
    "\n",
    "![Yet again the results are beyond underwhelming](imgs/dropout.png)\n",
    "\n",
    "The great feat of learning to play worse has finally been accomplished! Yay! Now on to the next task!\n",
    "\n",
    "#### Batch normalization, added layers\n",
    "- Added penalty for dying and increased reward for staying alive, clipping to prevent exploding gradients\n",
    "- Added batch normalization.\n",
    "- Use a larger network to potentially mitigare possible underfit or inability to comprehend the complexity:\n",
    "    - Added conv4 with 128 filters and kernel_size=3.\n",
    "    - Followed by batch normalization (bn4).\n",
    "    - Added fully connected layer fc3 with 256 units\n",
    "    - followed by batch normalization (fc_bn3).\n",
    "- Add Huber loss instead of smooth L1 loss for better stability\n",
    "- Double DQN to reduce overestimation bias: the policy network is now used for action selection, and the target network is used for Q-value evaluation.\n",
    "\n",
    "This will be the final model as time is up. Unfortunately the connection to the training that was supposed to be going on at Google Colab was interrupted, when I thought it was training. So several hours worth of computing didn't happen. As this was discovered merely 1.5h before deadline, the model will not have been trained for nearly as many episodes as desired.\n",
    "\n",
    "However, the initial results are still far from satisfactory. After 75 episodes the average score fluctuates around 160, which is better than many other versions of the model. The crux of the matter is yet to be penetrated.\n",
    "\n",
    "![Final model version with batch normalization and added layers](imgs/batch_normalization_added_layers.png)\n",
    "\n",
<<<<<<< HEAD
    "Still far from perfect. As of writing this, 100 episodes have been trained, which by all means also is far from enough. As the deadline will pass when training longer, I will include these preliminary findings, and update the repo after the deadline, with the results after having finished the training of the model, to see where it lands.\n",
    "\n",
    "UPDATE: after letting it run through the night, it seems to have lost touch very soon after leaving it (left it at 350 episodes). It ran for 615 episodes, and the results are yet again underwhelming. The results of the first model to be trained on Google Colab seems to have been the best one after all. Projecting the regression line into the future when running say the full intended 10.000 episodes could have yielded satisfactory results. One lesson here is then not to abort training too early to give the network a chance to learn, even though it's painfully slow.\n",
    "\n",
    "![Final model version with batch normalization and added layers, 600 eps](imgs/600episodes_final_model.png)"
=======
    "Still far from perfect. As of writing this, 100 episodes have been trained, which by all means also is far from enough. As the deadline will pass when training longer, I will include these preliminary findings, and update the repo after the deadline, with the results after having finished the training of the model, to see where it lands. "
>>>>>>> 5383ef7e2d8857e6106bafb72d13d93bfe106f6b
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
<<<<<<< HEAD
    "During the excersice I have had to grapple with many different aspects of neural networks, and started to get a feel for what it is all about. Many different techniques have been tried and considerations been made. All in all it has been a good learning experience, but it would also be nice to actually manage to find the root cause of the relatively low performance.\n",
    "\n",
    "The limitations of old hardware, even though it has 5.2 in CUDA compute, did make themselves well known. As did the cloud version, that several times lost touch due to both known and unknown causes and therefore stopped training."
=======
    "During the excersice I have had to grapple with many different aspects of neural networks, and started to get a feel for what it is all about. Many different techniques have been tried and considerations been made. All in all it has been a good learning experience, but it would also be nice to actually manage to find the root cause of the relatively low performance."
>>>>>>> 5383ef7e2d8857e6106bafb72d13d93bfe106f6b
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Below is the code for the latest model. It is preceded by the FrameStack code pulled from its source on github, as it was not available for import in any of the environments I tried:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FrameStack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this to have in memory, as importing FrameStack didn't work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Wrapper that stacks frames.\"\"\"\n",
    "from collections import deque\n",
    "from typing import Union\n",
    "\n",
    "import numpy as np\n",
    "# !pip install gymnasium[accept-rom-license]\n",
    "# !pip install gymnasium[atari]==0.28.1 ale-py==0.8.1\n",
    "# !pip install gymnasium[other]==0.28.1\n",
    "import gymnasium as gym\n",
    "from gymnasium.error import DependencyNotInstalled\n",
    "from gymnasium.spaces import Box\n",
    "\n",
    "\n",
    "class LazyFrames:\n",
    "    \"\"\"Ensures common frames are only stored once to optimize memory use.\n",
    "\n",
    "    To further reduce the memory use, it is optionally to turn on lz4 to compress the observations.\n",
    "\n",
    "    Note:\n",
    "        This object should only be converted to numpy array just before forward pass.\n",
    "    \"\"\"\n",
    "\n",
    "    __slots__ = (\"frame_shape\", \"dtype\", \"shape\", \"lz4_compress\", \"_frames\")\n",
    "\n",
    "    def __init__(self, frames: list, lz4_compress: bool = False):\n",
    "        \"\"\"Lazyframe for a set of frames and if to apply lz4.\n",
    "\n",
    "        Args:\n",
    "            frames (list): The frames to convert to lazy frames\n",
    "            lz4_compress (bool): Use lz4 to compress the frames internally\n",
    "\n",
    "        Raises:\n",
    "            DependencyNotInstalled: lz4 is not installed\n",
    "        \"\"\"\n",
    "        self.frame_shape = tuple(frames[0].shape)\n",
    "        self.shape = (len(frames),) + self.frame_shape\n",
    "        self.dtype = frames[0].dtype\n",
    "        if lz4_compress:\n",
    "            try:\n",
    "                from lz4.block import compress\n",
    "            except ImportError as e:\n",
    "                raise DependencyNotInstalled(\n",
    "                    \"lz4 is not installed, run `pip install gymnasium[other]`\"\n",
    "                ) from e\n",
    "\n",
    "            frames = [compress(frame) for frame in frames]\n",
    "        self._frames = frames\n",
    "        self.lz4_compress = lz4_compress\n",
    "\n",
    "    def __array__(self, dtype=None):\n",
    "        \"\"\"Gets a numpy array of stacked frames with specific dtype.\n",
    "\n",
    "        Args:\n",
    "            dtype: The dtype of the stacked frames\n",
    "\n",
    "        Returns:\n",
    "            The array of stacked frames with dtype\n",
    "        \"\"\"\n",
    "        arr = self[:]\n",
    "        if dtype is not None:\n",
    "            return arr.astype(dtype)\n",
    "        return arr\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the number of frame stacks.\n",
    "\n",
    "        Returns:\n",
    "            The number of frame stacks\n",
    "        \"\"\"\n",
    "        return self.shape[0]\n",
    "\n",
    "    def __getitem__(self, int_or_slice: Union[int, slice]):\n",
    "        \"\"\"Gets the stacked frames for a particular index or slice.\n",
    "\n",
    "        Args:\n",
    "            int_or_slice: Index or slice to get items for\n",
    "\n",
    "        Returns:\n",
    "            np.stacked frames for the int or slice\n",
    "\n",
    "        \"\"\"\n",
    "        if isinstance(int_or_slice, int):\n",
    "            return self._check_decompress(self._frames[int_or_slice])  # single frame\n",
    "        return np.stack(\n",
    "            [self._check_decompress(f) for f in self._frames[int_or_slice]], axis=0\n",
    "        )\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        \"\"\"Checks that the current frames are equal to the other object.\"\"\"\n",
    "        return self.__array__() == other\n",
    "\n",
    "    def _check_decompress(self, frame):\n",
    "        if self.lz4_compress:\n",
    "            from lz4.block import decompress\n",
    "\n",
    "            return np.frombuffer(decompress(frame), dtype=self.dtype).reshape(\n",
    "                self.frame_shape\n",
    "            )\n",
    "        return frame\n",
    "\n",
    "\n",
    "class FrameStack(gym.ObservationWrapper, gym.utils.RecordConstructorArgs):\n",
    "    \"\"\"Observation wrapper that stacks the observations in a rolling manner.\n",
    "\n",
    "    For example, if the number of stacks is 4, then the returned observation contains\n",
    "    the most recent 4 observations. For environment 'Pendulum-v1', the original observation\n",
    "    is an array with shape [3], so if we stack 4 observations, the processed observation\n",
    "    has shape [4, 3].\n",
    "\n",
    "    Note:\n",
    "        - To be memory efficient, the stacked observations are wrapped by :class:`LazyFrame`.\n",
    "        - The observation space must be :class:`Box` type. If one uses :class:`Dict`\n",
    "          as observation space, it should apply :class:`FlattenObservation` wrapper first.\n",
    "        - After :meth:`reset` is called, the frame buffer will be filled with the initial observation.\n",
    "          I.e. the observation returned by :meth:`reset` will consist of `num_stack` many identical frames.\n",
    "\n",
    "    Example:\n",
    "        >>> import gymnasium as gym\n",
    "        >>> from gymnasium.wrappers import FrameStack\n",
    "        >>> env = gym.make(\"CarRacing-v2\")\n",
    "        >>> env = FrameStack(env, 4)\n",
    "        >>> env.observation_space\n",
    "        Box(0, 255, (4, 96, 96, 3), uint8)\n",
    "        >>> obs, _ = env.reset()\n",
    "        >>> obs.shape\n",
    "        (4, 96, 96, 3)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        env: gym.Env,\n",
    "        num_stack: int,\n",
    "        lz4_compress: bool = False,\n",
    "    ):\n",
    "        \"\"\"Observation wrapper that stacks the observations in a rolling manner.\n",
    "\n",
    "        Args:\n",
    "            env (Env): The environment to apply the wrapper\n",
    "            num_stack (int): The number of frames to stack\n",
    "            lz4_compress (bool): Use lz4 to compress the frames internally\n",
    "        \"\"\"\n",
    "        gym.utils.RecordConstructorArgs.__init__(\n",
    "            self, num_stack=num_stack, lz4_compress=lz4_compress\n",
    "        )\n",
    "        gym.ObservationWrapper.__init__(self, env)\n",
    "\n",
    "        self.num_stack = num_stack\n",
    "        self.lz4_compress = lz4_compress\n",
    "\n",
    "        self.frames = deque(maxlen=num_stack)\n",
    "\n",
    "        low = np.repeat(self.observation_space.low[np.newaxis, ...], num_stack, axis=0)\n",
    "        high = np.repeat(\n",
    "            self.observation_space.high[np.newaxis, ...], num_stack, axis=0\n",
    "        )\n",
    "        self.observation_space = Box(\n",
    "            low=low, high=high, dtype=self.observation_space.dtype\n",
    "        )\n",
    "\n",
    "    def observation(self, observation):\n",
    "        \"\"\"Converts the wrappers current frames to lazy frames.\n",
    "\n",
    "        Args:\n",
    "            observation: Ignored\n",
    "\n",
    "        Returns:\n",
    "            :class:`LazyFrames` object for the wrapper's frame buffer,  :attr:`self.frames`\n",
    "        \"\"\"\n",
    "        assert len(self.frames) == self.num_stack, (len(self.frames), self.num_stack)\n",
    "        return LazyFrames(list(self.frames), self.lz4_compress)\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Steps through the environment, appending the observation to the frame buffer.\n",
    "\n",
    "        Args:\n",
    "            action: The action to step through the environment with\n",
    "\n",
    "        Returns:\n",
    "            Stacked observations, reward, terminated, truncated, and information from the environment\n",
    "        \"\"\"\n",
    "        observation, reward, terminated, truncated, info = self.env.step(action)\n",
    "        self.frames.append(observation)\n",
    "        return self.observation(None), reward, terminated, truncated, info\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        \"\"\"Reset the environment with kwargs.\n",
    "\n",
    "        Args:\n",
    "            **kwargs: The kwargs for the environment reset\n",
    "\n",
    "        Returns:\n",
    "            The stacked observations\n",
    "        \"\"\"\n",
    "        obs, info = self.env.reset(**kwargs)\n",
    "\n",
    "        [self.frames.append(obs) for _ in range(self.num_stack)]\n",
    "\n",
    "        return self.observation(None), info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: gymnasium[accept-rom-license] in c:\\users\\mikae\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.0.0)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\mikae\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gymnasium[accept-rom-license]) (1.24.3)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\mikae\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gymnasium[accept-rom-license]) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\mikae\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gymnasium[accept-rom-license]) (4.12.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\mikae\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gymnasium[accept-rom-license]) (0.0.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: gymnasium 1.0.0 does not provide the extra 'accept-rom-license'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading replay buffer from disk...\n",
      "Replay buffer loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mikae\\AppData\\Local\\Temp\\ipykernel_3040\\1140974223.py:75: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint loaded from checkpoint.pth\n",
      "Starting torch.save for checkpoint...\n",
      "Checkpoint saved at checkpoint.pth\n",
      "Ep: 325\tPoints: 90.00\tLoss: 0.570368\tEps: 0.665388\tFrames: 30196\tTime: 28.74s\n",
      "Ep: 326\tPoints: 115.00\tLoss: 0.272040\tEps: 0.664466\tFrames: 30798\tTime: 22.98s\n",
      "Ep: 327\tPoints: 90.00\tLoss: 0.261075\tEps: 0.663884\tFrames: 31179\tTime: 14.74s\n",
      "Ep: 328\tPoints: 65.00\tLoss: 0.266668\tEps: 0.663442\tFrames: 31468\tTime: 10.72s\n",
      "Ep: 329\tPoints: 420.00\tLoss: 0.252884\tEps: 0.662133\tFrames: 32326\tTime: 33.92s\n",
      "Ep: 330\tPoints: 150.00\tLoss: 0.268871\tEps: 0.661554\tFrames: 32706\tTime: 14.28s\n",
      "Ep: 331\tPoints: 30.00\tLoss: 0.203051\tEps: 0.661107\tFrames: 32999\tTime: 11.93s\n",
      "Ep: 332\tPoints: 125.00\tLoss: 0.227904\tEps: 0.660388\tFrames: 33472\tTime: 19.02s\n",
      "Ep: 333\tPoints: 110.00\tLoss: 0.190584\tEps: 0.659651\tFrames: 33957\tTime: 19.96s\n",
      "Ep: 334\tPoints: 300.00\tLoss: 0.209094\tEps: 0.658590\tFrames: 34656\tTime: 27.04s\n",
      "Ep: 335\tPoints: 210.00\tLoss: 0.239093\tEps: 0.657638\tFrames: 35284\tTime: 24.43s\n",
      "Ep: 336\tPoints: 220.00\tLoss: 0.213835\tEps: 0.656748\tFrames: 35872\tTime: 23.94s\n",
      "Ep: 337\tPoints: 110.00\tLoss: 0.220960\tEps: 0.655964\tFrames: 36391\tTime: 21.31s\n",
      "Ep: 338\tPoints: 210.00\tLoss: 0.233447\tEps: 0.655147\tFrames: 36932\tTime: 19.59s\n",
      "Ep: 339\tPoints: 210.00\tLoss: 0.245689\tEps: 0.654386\tFrames: 37437\tTime: 18.05s\n",
      "Ep: 340\tPoints: 135.00\tLoss: 0.245076\tEps: 0.653791\tFrames: 37832\tTime: 14.20s\n",
      "Ep: 341\tPoints: 155.00\tLoss: 0.250125\tEps: 0.653075\tFrames: 38308\tTime: 17.05s\n",
      "Ep: 342\tPoints: 160.00\tLoss: 0.242067\tEps: 0.652344\tFrames: 38794\tTime: 17.76s\n",
      "Ep: 343\tPoints: 105.00\tLoss: 0.240330\tEps: 0.651634\tFrames: 39267\tTime: 17.32s\n",
      "Ep: 344\tPoints: 440.00\tLoss: 0.244565\tEps: 0.650074\tFrames: 40308\tTime: 41.72s\n",
      "Ep: 345\tPoints: 155.00\tLoss: 0.274960\tEps: 0.649392\tFrames: 40764\tTime: 17.39s\n",
      "Ep: 346\tPoints: 50.00\tLoss: 0.293084\tEps: 0.648950\tFrames: 41060\tTime: 11.92s\n",
      "Ep: 347\tPoints: 210.00\tLoss: 0.290608\tEps: 0.647825\tFrames: 41813\tTime: 29.98s\n",
      "Ep: 348\tPoints: 260.00\tLoss: 0.328044\tEps: 0.646998\tFrames: 42368\tTime: 22.72s\n",
      "Ep: 349\tPoints: 75.00\tLoss: 0.279848\tEps: 0.646572\tFrames: 42654\tTime: 11.60s\n",
      "Starting torch.save for checkpoint...\n",
      "Checkpoint saved at checkpoint.pth\n",
      "Ep: 350\tPoints: 405.00\tLoss: 0.318158\tEps: 0.645130\tFrames: 43624\tTime: 38.32s\n",
      "Ep: 351\tPoints: 280.00\tLoss: 0.342314\tEps: 0.644043\tFrames: 44356\tTime: 31.15s\n",
      "Ep: 352\tPoints: 90.00\tLoss: 0.408362\tEps: 0.643477\tFrames: 44738\tTime: 14.55s\n",
      "Ep: 353\tPoints: 210.00\tLoss: 0.360886\tEps: 0.642550\tFrames: 45364\tTime: 23.57s\n",
      "Ep: 354\tPoints: 110.00\tLoss: 0.367162\tEps: 0.641919\tFrames: 45791\tTime: 16.29s\n",
      "Ep: 355\tPoints: 180.00\tLoss: 0.356788\tEps: 0.641007\tFrames: 46408\tTime: 23.23s\n",
      "Ep: 356\tPoints: 300.00\tLoss: 0.392000\tEps: 0.639785\tFrames: 47237\tTime: 32.70s\n",
      "NaN or Inf encountered in loss. Skipping step.\n",
      "NaN or Inf encountered in loss. Skipping step.\n",
      "Starting torch.save for checkpoint...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 218\u001b[0m\n\u001b[0;32m    216\u001b[0m     loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msmooth_l1_loss(q_values, target_q_values)\n\u001b[1;32m--> 218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misfinite(loss):\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNaN or Inf encountered in loss. Skipping step.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 258\u001b[0m\n\u001b[0;32m    255\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining interrupted by user.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     \u001b[38;5;66;03m# Save final checkpoint with episode metrics\u001b[39;00m\n\u001b[1;32m--> 258\u001b[0m     \u001b[43msave_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpolicy_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_count\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisode_points\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisode_losses\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    259\u001b[0m     torch\u001b[38;5;241m.\u001b[39msave(policy_net\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdqn_space_invaders_final.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[2], line 70\u001b[0m, in \u001b[0;36msave_checkpoint\u001b[1;34m(model, optimizer, episode, epsilon, total_steps, frame_count, episode_points, episode_losses, checkpoint_path)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting torch.save for checkpoint...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     60\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_state_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m: model\u001b[38;5;241m.\u001b[39mstate_dict(),\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizer_state_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m: optimizer\u001b[38;5;241m.\u001b[39mstate_dict(),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepisode_losses\u001b[39m\u001b[38;5;124m\"\u001b[39m: episode_losses\n\u001b[0;32m     69\u001b[0m }\n\u001b[1;32m---> 70\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCheckpoint saved at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Mikae\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\serialization.py:652\u001b[0m, in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[0;32m    650\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[0;32m    651\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m--> 652\u001b[0m         \u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_disable_byteorder_record\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    653\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    654\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Mikae\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\serialization.py:883\u001b[0m, in \u001b[0;36m_save\u001b[1;34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[0m\n\u001b[0;32m    879\u001b[0m \u001b[38;5;66;03m# given that we copy things around anyway, we might use storage.cpu()\u001b[39;00m\n\u001b[0;32m    880\u001b[0m \u001b[38;5;66;03m# this means to that to get tensors serialized, you need to implement\u001b[39;00m\n\u001b[0;32m    881\u001b[0m \u001b[38;5;66;03m# .cpu() on the underlying Storage\u001b[39;00m\n\u001b[0;32m    882\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m storage\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 883\u001b[0m     storage \u001b[38;5;241m=\u001b[39m \u001b[43mstorage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    884\u001b[0m \u001b[38;5;66;03m# Now that it is on the CPU we can directly copy it into the zip file\u001b[39;00m\n\u001b[0;32m    885\u001b[0m num_bytes \u001b[38;5;241m=\u001b[39m storage\u001b[38;5;241m.\u001b[39mnbytes()\n",
      "File \u001b[1;32mc:\\Users\\Mikae\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\storage.py:167\u001b[0m, in \u001b[0;36m_StorageBase.cpu\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return a CPU copy of this storage if it's not already on the CPU.\"\"\"\u001b[39;00m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 167\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mUntypedStorage\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "%pip install gymnasium[accept-rom-license]\n",
    "%pip install ale-py\n",
    "import gymnasium as gym\n",
    "import ale_py\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import csv\n",
    "from collections import deque\n",
    "from gymnasium.wrappers.atari_preprocessing import AtariPreprocessing\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Enable cuDNN auto-tuner for better performance in Colab\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, num_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(4, 32, kernel_size=8, stride=4)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.conv4 = nn.Conv2d(128, 128, kernel_size=3, stride=1)\n",
    "        self.bn4 = nn.BatchNorm2d(128)\n",
    "\n",
    "        # Compute the size of the flattened layer dynamically\n",
    "        self._initialize_fc_input_shape()\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(self.fc_input_dim, 1024)\n",
    "        self.fc_bn1 = nn.BatchNorm1d(1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.fc_bn2 = nn.BatchNorm1d(512)\n",
    "        self.fc3 = nn.Linear(512, 256)\n",
    "        self.fc_bn3 = nn.BatchNorm1d(256)\n",
    "        self.fc4 = nn.Linear(256, num_actions)\n",
    "\n",
    "    def _initialize_fc_input_shape(self):\n",
    "        # Use a dummy input to determine the size of the flattened layer\n",
    "        dummy_input = torch.zeros(1, 4, 84, 84)\n",
    "        with torch.no_grad():\n",
    "            x = F.relu(self.bn1(self.conv1(dummy_input)))\n",
    "            x = F.relu(self.bn2(self.conv2(x)))\n",
    "            x = F.relu(self.bn3(self.conv3(x)))\n",
    "            x = F.relu(self.bn4(self.conv4(x)))\n",
    "        self.fc_input_dim = x.numel()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = F.relu(self.fc_bn1(self.fc1(x)))\n",
    "        x = F.relu(self.fc_bn2(self.fc2(x)))\n",
    "        x = F.relu(self.fc_bn3(self.fc3(x)))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class PrioritizedGPUReplayBuffer:\n",
    "    def __init__(self, capacity, alpha=0.6):\n",
    "        self.capacity = capacity\n",
    "        self.index = 0\n",
    "        self.full = False\n",
    "        self.alpha = alpha\n",
    "\n",
    "        self.states = torch.zeros((capacity, 4, 84, 84), dtype=torch.float32, device=device)\n",
    "        self.actions = torch.zeros((capacity, 1), dtype=torch.int64, device=device)\n",
    "        self.rewards = torch.zeros((capacity, 1), dtype=torch.float32, device=device)\n",
    "        self.next_states = torch.zeros((capacity, 4, 84, 84), dtype=torch.float32, device=device)\n",
    "        self.dones = torch.zeros((capacity, 1), dtype=torch.bool, device=device)\n",
    "        self.priorities = torch.zeros((capacity,), dtype=torch.float32, device=device)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        max_priority = self.priorities.max().item() if self.index > 0 else 1.0\n",
    "\n",
    "        self.states[self.index] = torch.tensor(state, dtype=torch.float32, device=device)\n",
    "        self.actions[self.index] = torch.tensor([action], dtype=torch.int64, device=device)\n",
    "        self.rewards[self.index] = torch.tensor([reward], dtype=torch.float32, device=device)\n",
    "        self.next_states[self.index] = torch.tensor(next_state, dtype=torch.float32, device=device)\n",
    "        self.dones[self.index] = torch.tensor([done], dtype=torch.bool, device=device)\n",
    "        self.priorities[self.index] = max_priority\n",
    "\n",
    "        self.index = (self.index + 1) % self.capacity\n",
    "        if self.index == 0:\n",
    "            self.full = True\n",
    "\n",
    "    def sample(self, batch_size, beta=0.4):\n",
    "        max_index = self.capacity if self.full else self.index\n",
    "        priorities = self.priorities[:max_index]\n",
    "        probabilities = priorities ** self.alpha\n",
    "        probabilities /= probabilities.sum()\n",
    "\n",
    "        indices = torch.multinomial(probabilities, batch_size, replacement=False)\n",
    "\n",
    "        weights = (max_index * probabilities[indices]) ** (-beta)\n",
    "        weights /= weights.max()\n",
    "        weights = weights.to(device)\n",
    "\n",
    "        return (\n",
    "            self.states[indices],\n",
    "            self.actions[indices],\n",
    "            self.rewards[indices],\n",
    "            self.next_states[indices],\n",
    "            self.dones[indices],\n",
    "            weights,\n",
    "            indices\n",
    "        )\n",
    "\n",
    "    def update_priorities(self, batch_indices, batch_priorities):\n",
    "        for idx, priority in zip(batch_indices, batch_priorities):\n",
    "            self.priorities[idx] = priority\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.capacity if self.full else self.index\n",
    "\n",
    "def shape_reward(reward, done, time_alive):\n",
    "    \"\"\"\n",
    "    Enhanced reward shaping for Space Invaders\n",
    "    - Squared reward scaling for better gradient signals\n",
    "    - Death penalty scales with time alive\n",
    "    - Small constant reward for survival\n",
    "    - Clipped to prevent exploding gradients\n",
    "    \n",
    "    Args:\n",
    "        reward (float): Original game reward\n",
    "        done (bool): Whether the episode ended\n",
    "        time_alive (int): Number of frames the agent has survived\n",
    "    \"\"\"\n",
    "    if reward > 0:\n",
    "        # Square positive rewards (scoring points) to emphasize good actions\n",
    "        shaped_reward = (reward / 10.0) ** 2 + 0.5\n",
    "    else:\n",
    "        shaped_reward = reward\n",
    "\n",
    "    survival_bonus = min(0.01 * (time_alive / 1000), 0.05)\n",
    "    shaped_reward += survival_bonus\n",
    "    \n",
    "    if done:\n",
    "        death_penalty = max(-2.0, -2.0 + (time_alive / 5000))\n",
    "        shaped_reward += death_penalty\n",
    "    \n",
    "    # Clip to prevent extreme values\n",
    "    return np.clip(shaped_reward, -2.0, 2.0)\n",
    "\n",
    "def save_checkpoint(model, optimizer, episode, epsilon, total_steps, frame_count, episode_points, episode_losses, checkpoint_path=\"checkpoint.pth\"):\n",
    "    print(\"Starting torch.save for checkpoint...\")\n",
    "    checkpoint = {\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"episode\": episode,\n",
    "        \"epsilon\": epsilon,\n",
    "        \"total_steps\": total_steps,\n",
    "        \"frame_count\": frame_count,\n",
    "        \"episode_points\": episode_points,\n",
    "        \"episode_losses\": episode_losses\n",
    "    }\n",
    "    torch.save(checkpoint, checkpoint_path)\n",
    "    print(f\"Checkpoint saved at {checkpoint_path}\")\n",
    "\n",
    "def load_checkpoint(model, optimizer, checkpoint_path=\"checkpoint.pth\"):\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "        episode = checkpoint[\"episode\"]\n",
    "        epsilon = checkpoint[\"epsilon\"]\n",
    "        total_steps = checkpoint[\"total_steps\"]\n",
    "        frame_count = checkpoint.get(\"frame_count\", 0)\n",
    "        episode_points = checkpoint.get(\"episode_points\", [])\n",
    "        episode_losses = checkpoint.get(\"episode_losses\", [])\n",
    "        print(f\"Checkpoint loaded from {checkpoint_path}\")\n",
    "        return episode, epsilon, total_steps, frame_count, episode_points, episode_losses\n",
    "    else:\n",
    "        print(\"No checkpoint found, starting from scratch.\")\n",
    "        return 0, 1.0, 0, 0, [], []\n",
    "\n",
    "def prefill_replay_buffer(env, memory, prefill_size, policy_net, optimizer, gamma, scaler, batch_size=64):\n",
    "    print(f\"Prefilling replay buffer with {prefill_size} transitions...\")\n",
    "    state, _ = env.reset()\n",
    "    for _ in tqdm(range(prefill_size), desc=\"Filling Replay Buffer\"):\n",
    "        state_array = np.array(state)\n",
    "        if state_array.shape == (84, 84, 4):\n",
    "            state_array = state_array.transpose(2, 0, 1)\n",
    "\n",
    "        action = env.action_space.sample()\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        next_state_array = np.array(next_state)\n",
    "        if next_state_array.shape == (84, 84, 4):\n",
    "            next_state_array = next_state_array.transpose(2, 0, 1)\n",
    "\n",
    "        reward = shape_reward(reward, done, 0)\n",
    "\n",
    "        memory.push(state_array / 255.0, action, reward, next_state_array / 255.0, done)\n",
    "\n",
    "        if len(memory) >= batch_size:\n",
    "            states, actions, rewards, next_states, dones, weights, indices = memory.sample(batch_size)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                next_actions = policy_net(next_states).argmax(dim=1, keepdim=True)\n",
    "                next_q_values = policy_net(next_states).gather(1, next_actions)\n",
    "                target_q_values = rewards + gamma * next_q_values * (1 - dones.float())\n",
    "\n",
    "            with torch.cuda.amp.autocast():\n",
    "                q_values = policy_net(states).gather(1, actions)\n",
    "                loss = (weights * F.huber_loss(q_values, target_q_values, reduction='none')).mean()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            scaler.scale(loss).backward()\n",
    "            torch.nn.utils.clip_grad_norm_(policy_net.parameters(), 1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            priorities = (torch.abs(q_values - target_q_values) + 1e-5).detach()\n",
    "            memory.update_priorities(indices, priorities)\n",
    "\n",
    "        if done:\n",
    "            state, _ = env.reset()\n",
    "        else:\n",
    "            state = next_state\n",
    "\n",
    "    print(\"Replay buffer prefilled.\")\n",
    "\n",
    "num_actions = 6\n",
    "batch_size = 64\n",
    "gamma = 0.99\n",
    "epsilon_start = 1.0\n",
    "epsilon_end = 0.1\n",
    "frames_to_decay = 2_000_000\n",
    "epsilon_decay = (epsilon_end / epsilon_start) ** (1 / frames_to_decay)\n",
    "\n",
    "target_update_freq = 1000\n",
    "memory_size = 100000\n",
    "num_episodes = 3000\n",
    "save_frequency = 100\n",
    "updates_per_step = 4\n",
    "beta_start = 0.4\n",
    "beta_frames = 1_000_000\n",
    "learning_rate = 1e-4\n",
    "\n",
    "frame_count = 0\n",
    "env = gym.make(\"SpaceInvadersNoFrameskip-v4\")\n",
    "env = AtariPreprocessing(env)\n",
    "env = FrameStack(env, 4)\n",
    "\n",
    "policy_net = DQN(num_actions).to(device)\n",
    "target_net = DQN(num_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "memory = PrioritizedGPUReplayBuffer(memory_size)\n",
    "prefill_size = batch_size * 500\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "prefill_replay_buffer(env, memory, prefill_size, policy_net, optimizer, gamma, scaler)\n",
    "\n",
    "start_episode, epsilon, total_steps, frame_count, episode_points, episode_losses = load_checkpoint(policy_net, optimizer, \"checkpoint.pth\")\n",
    "\n",
    "total_score = 0\n",
    "\n",
    "csv_file = \"training_results.csv\"\n",
    "if not os.path.exists(csv_file):\n",
    "    with open(csv_file, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"Episode\", \"Score\", \"Loss\", \"Epsilon\", \"Frames\", \"Avg Score\"])\n",
    "\n",
    "try:\n",
    "    for episode in range(start_episode, num_episodes):\n",
    "        start_time = time.time()\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        episode_loss = 0\n",
    "        smoothed_loss = None\n",
    "        updates = 0\n",
    "        time_alive = 0\n",
    "        beta = min(1.0, beta_start + frame_count * (1.0 - beta_start) / beta_frames)\n",
    "\n",
    "        while not done:\n",
    "            state_array = np.array(state)\n",
    "            if state_array.shape == (84, 84, 4):\n",
    "                state_array = state_array.transpose(2, 0, 1)\n",
    "\n",
    "            if random.random() < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                state_tensor = torch.tensor(state_array / 255.0, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "                policy_net.eval()  # Switch to evaluation mode for inference\n",
    "                with torch.no_grad():\n",
    "                    action_values = policy_net(state_tensor)\n",
    "                policy_net.train()  # Switch back to training mode\n",
    "                action = torch.argmax(action_values, dim=1).item()\n",
    "\n",
    "\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            total_reward += reward\n",
    "            frame_count += 1\n",
    "            time_alive += 1\n",
    "\n",
    "            next_state_array = np.array(next_state)\n",
    "            if next_state_array.shape == (84, 84, 4):\n",
    "                next_state_array = next_state_array.transpose(2, 0, 1)\n",
    "            \n",
    "            reward = shape_reward(reward, done, time_alive)\n",
    "\n",
    "            memory.push(state_array / 255.0, action, reward, next_state_array / 255.0, done)\n",
    "\n",
    "            for _ in range(updates_per_step):\n",
    "                if len(memory) >= batch_size:\n",
    "                    states, actions, rewards, next_states, dones, weights, indices = memory.sample(batch_size, beta)\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        next_actions = policy_net(next_states).argmax(dim=1, keepdim=True)\n",
    "                        next_q_values = target_net(next_states).gather(1, next_actions)\n",
    "                    target_q_values = rewards + gamma * next_q_values * (1 - dones.float())\n",
    "\n",
    "                    with torch.cuda.amp.autocast():\n",
    "                        q_values = policy_net(states).gather(1, actions)\n",
    "                        loss = (weights * F.huber_loss(q_values, target_q_values, reduction='none')).mean()\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    scaler.scale(loss).backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(policy_net.parameters(), 1.0)\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "\n",
    "                    priorities = (torch.abs(q_values - target_q_values) + 1e-5).detach()\n",
    "                    memory.update_priorities(indices, priorities)\n",
    "\n",
    "                    episode_loss += loss.item()\n",
    "                    updates += 1\n",
    "\n",
    "            epsilon = max(epsilon_end, epsilon * epsilon_decay)\n",
    "            if frame_count % target_update_freq == 0:\n",
    "                target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        average_loss = episode_loss / updates if updates > 0 else 0.0\n",
    "        if smoothed_loss is None:\n",
    "            smoothed_loss = average_loss\n",
    "        else:\n",
    "            smoothing_factor = 0.9\n",
    "            smoothed_loss = smoothing_factor * smoothed_loss + (1 - smoothing_factor) * average_loss\n",
    "\n",
    "        total_score += total_reward\n",
    "        avg_score = total_score / (episode + 1)\n",
    "\n",
    "        episode_points.append(total_reward)\n",
    "        episode_losses.append(average_loss)\n",
    "\n",
    "        with open(csv_file, mode='a', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([episode, total_reward, smoothed_loss, epsilon, frame_count, avg_score])\n",
    "\n",
    "        if episode % save_frequency == 0:\n",
    "            save_checkpoint(policy_net, optimizer, episode, epsilon, total_steps, frame_count, episode_points, episode_losses)\n",
    "\n",
    "        print(f\"Ep: {episode:4d} Score: {total_reward:8.2f} Smoothed Huber Loss: {smoothed_loss:10.6f} \"\n",
    "              f\"Epsilon: {epsilon:8.6f} Frames: {frame_count:10d} Time: {time.time() - start_time:8.2f}s \"\n",
    "              f\"Avg Score: {avg_score:8.2f}\")\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Training interrupted by user.\")\n",
    "finally:\n",
    "    save_checkpoint(policy_net, optimizer, num_episodes, epsilon, total_steps, frame_count, episode_points, episode_losses)\n",
    "    print(\"Final checkpoint saved. Training complete.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As evidenced above, there were issues with my old NVIDIA GPU that led to me finally abandoning training it locally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mikae\\AppData\\Local\\Temp\\ipykernel_8140\\826240131.py:30: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  agent.load_state_dict(torch.load('dqn_space_invaders.pth', map_location=device))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DQN(\n",
       "  (conv1): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
       "  (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (fc1): Linear(in_features=3136, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Number of actions in Space Invaders\n",
    "num_actions = 6\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, num_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(4, 32, kernel_size=8, stride=4)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.conv4 = nn.Conv2d(128, 128, kernel_size=3, stride=1)\n",
    "        self.bn4 = nn.BatchNorm2d(128)\n",
    "\n",
    "        # Compute the size of the flattened layer dynamically\n",
    "        self._initialize_fc_input_shape()\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(self.fc_input_dim, 1024)\n",
    "        self.fc_bn1 = nn.BatchNorm1d(1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.fc_bn2 = nn.BatchNorm1d(512)\n",
    "        self.fc3 = nn.Linear(512, 256)\n",
    "        self.fc_bn3 = nn.BatchNorm1d(256)\n",
    "        self.fc4 = nn.Linear(256, num_actions)\n",
    "\n",
    "    def _initialize_fc_input_shape(self):\n",
    "        # Use a dummy input to determine the size of the flattened layer\n",
    "        dummy_input = torch.zeros(1, 4, 84, 84)\n",
    "        with torch.no_grad():\n",
    "            x = F.relu(self.bn1(self.conv1(dummy_input)))\n",
    "            x = F.relu(self.bn2(self.conv2(x)))\n",
    "            x = F.relu(self.bn3(self.conv3(x)))\n",
    "            x = F.relu(self.bn4(self.conv4(x)))\n",
    "        self.fc_input_dim = x.numel()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = F.relu(self.fc_bn1(self.fc1(x)))\n",
    "        x = F.relu(self.fc_bn2(self.fc2(x)))\n",
    "        x = F.relu(self.fc_bn3(self.fc3(x)))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Initialize the model and load the trained weights\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "agent = DQN(num_actions).to(device)\n",
    "agent.load_state_dict(torch.load('checkpoint.pth', map_location=device))\n",
    "agent.eval()  # Set the model to evaluation mode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up env with rendering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import ale_py\n",
    "from gymnasium.wrappers.atari_preprocessing import AtariPreprocessing\n",
    "\n",
    "# Create the environment with rendering enabled\n",
    "env = gym.make(\"SpaceInvadersNoFrameskip-v4\", render_mode=\"human\")\n",
    "env = AtariPreprocessing(env)\n",
    "env = FrameStack(env, num_stack=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the agent in the env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "state, _ = env.reset()\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        action_values = agent(state_tensor)\n",
    "        action = torch.argmax(action_values, dim=1).item()\n",
    "    \n",
    "    next_state, reward, done, _, _ = env.step(action)\n",
    "    \n",
    "    state = next_state\n",
    "    \n",
    "    env.render()\n",
    "    \n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with recording\n",
    "\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "\n",
    "env = gym.make(\"SpaceInvadersNoFrameskip-v4\")\n",
    "env = AtariPreprocessing(env)\n",
    "env = FrameStack(env, 4)\n",
    "\n",
    "# Wrap the environment to record videos\n",
    "env = RecordVideo(env, video_folder='videos', episode_trigger=lambda x: True)\n",
    "\n",
    "state, _ = env.reset()\n",
    "done = False\n",
    "total_reward = 0\n",
    "\n",
    "while not done:\n",
    "    state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Get action from the trained model\n",
    "        action_values = agent(state_tensor)\n",
    "        action = torch.argmax(action_values, dim=1).item()\n",
    "    \n",
    "    next_state, reward, done, _, _ = env.step(action)\n",
    "    total_reward += reward\n",
    "    state = next_state\n",
    "\n",
    "env.close()\n",
    "print(f\"Total Reward: {total_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "# Function to display the video\n",
    "def show_video():\n",
    "    video_files = [f for f in os.listdir('./videos') if f.endswith('.mp4')]\n",
    "    if len(video_files) > 0:\n",
    "        video_path = os.path.join('./videos', video_files[-1])\n",
    "        video = io.open(video_path, 'r+b').read()\n",
    "        encoded = base64.b64encode(video)\n",
    "        display(HTML(data='''\n",
    "            <video width=\"480\" height=\"320\" controls>\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "            </video>\n",
    "            '''.format(encoded.decode('ascii'))))\n",
    "    else:\n",
    "        print(\"No video found.\")\n",
    "\n",
    "# Display the recorded video\n",
    "show_video()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
